{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up code checking\n",
    "# Set up filepaths\n",
    "import os\n",
    "os.chdir(os.path.join(os.path.expanduser('~'), 'kaggle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join('data', 'house-price-data', 'train.csv')\n",
    "train_data = pd.read_csv(train_path)\n",
    "\n",
    "test_path = os.path.join('data', 'house-price-data', 'test.csv')\n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select numerical and categorical variables, and separate target from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n",
      "['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n"
     ]
    }
   ],
   "source": [
    "y = train_data['SalePrice']\n",
    "train_data.drop(columns=['SalePrice'], inplace=True)\n",
    "\n",
    "numerical_columns = train_data.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "numerical_columns.remove('MSSubClass')\n",
    "\n",
    "categorical_columns = [c for c in train_data.columns if c not in numerical_columns]\n",
    "\n",
    "print(numerical_columns)\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add missing data col for every column with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# adds additional column which indicates whether data was missing for given feature\n",
    "class AddMissingIndicator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_columns):\n",
    "        self.numeric_columns = numeric_columns\n",
    "        self.num_cols_with_na = [] \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.num_cols_with_na = [c for c in self.numeric_columns if X[c].isna().any()]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        for c in self.num_cols_with_na:\n",
    "            missing_col_name = f\"{c}_is_missing\"\n",
    "            X[missing_col_name] = np.where(X[c].isna(), 1, 0)\n",
    "        \n",
    "        return X \n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')), \n",
    "    ('OHE', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('missing_indicator', AddMissingIndicator(numerical_columns)),\n",
    "    ('imputer', SimpleImputer(strategy='mean')), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer(transformers=[\n",
    "    ('numerical', num_pipeline, numerical_columns),\n",
    "    ('categorical', cat_pipeline, categorical_columns)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ridge\n",
      "Mae: 17504.386380929947\n",
      "Mae, relative: 0.10096311312030672\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasil/earth-analytics-parent/earth-analytics/conda-env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.474e+11, tolerance: 7.806e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: lasso\n",
      "Mae: 17200.36663236229\n",
      "Mae, relative: 0.0992095652039465\n",
      "\n",
      "Model: xgboost\n",
      "Mae: 15723.801423373288\n",
      "Mae, relative: 0.09069292160499814\n",
      "\n",
      "Model: GBR\n",
      "Mae: 14641.809863420407\n",
      "Mae, relative: 0.08445212950378218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "models = {\n",
    "    'ridge': Ridge(),\n",
    "    'lasso': Lasso(random_state=1),\n",
    "    'xgboost': XGBRegressor(random_state=1, eval_metric='mae'),\n",
    "    'GBR': GradientBoostingRegressor(loss='absolute_error', random_state=1),\n",
    "}\n",
    "\n",
    "pipelines = {key: Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    (f'{key}', models[key])\n",
    "]) for key in models}\n",
    "\n",
    "def get_mae(predictor, X_train, y_train, X_test, y_test):\n",
    "    predictor.fit(X_train, y_train)\n",
    "    predictions = predictor.predict(X_test)\n",
    "    return mean_absolute_error(predictions, y_test)\n",
    "\n",
    "\n",
    "for p in pipelines:\n",
    "    predictor = pipelines[p]\n",
    "    mae = get_mae(predictor, X_train, y_train, X_test, y_test)\n",
    "    print(f\"Model: {p}\")\n",
    "    print(f'Mae: {mae}')\n",
    "    print(f'Mae, relative: {mae/np.mean(y_test)}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV parameters for models with small number of params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_small_model = {}\n",
    "\n",
    "param_grid_small_model['ridge'] = {\n",
    "    'ridge__alpha': np.logspace(1.5, 1.8, num=11),\n",
    "    'ridge__tol': np.logspace(-6, -5, num=11)\n",
    "}\n",
    "\n",
    "# param_grid_small_model['lasso'] = {\n",
    "#     'lasso__alpha': np.logspace(-1, 1, num=11),\n",
    "#     'lasso__max_iter': [1000],\n",
    "#     'lasso__tol': np.logspace(-4, -3.5, num=2)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "for model_name in param_grid_small_model:\n",
    "    print(f\"Model name: {model_name}\")\n",
    "    pipe = pipelines[model_name]\n",
    "    search = GridSearchCV(estimator=pipe, param_grid=param_grid_small_model[model_name], scoring='neg_mean_absolute_error')\n",
    "    search.fit(train_data, y)\n",
    "    print(f'Best values: {search.best_params_}')\n",
    "    print(f'Best score MAE: {search.best_score_}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search for models with many params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_param_grids = {}\n",
    "consecutive_param_grids['xgboost'] = [ \n",
    "    {\n",
    "        'xgboost__max_depth':range(3,8),\n",
    "        'xgboost__min_child_weight':range(1,5,1), \n",
    "        'xgboost__eval_metric': ['mae']\n",
    "    }, \n",
    "    {\n",
    "        'xgboost__gamma':[i/10.0 for i in range(0,5)]\n",
    "    },\n",
    "    {\n",
    "        'xgboost__subsample':[i/10.0 for i in range(6,11)],\n",
    "        'xgboost__colsample_bytree':[i/10.0 for i in range(6,11)]\n",
    "    }, \n",
    "    {\n",
    "        'xgboost__reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "consecutive_param_grids['GBR'] = [\n",
    "    {\n",
    "        'GBR__n_estimators': range(300, 500, 30), \n",
    "        'GBR__subsample': [1],\n",
    "        'GBR__loss': ['absolute_error'],\n",
    "        'GBR__random_state': [1]\n",
    "    }, \n",
    "    {\n",
    "        'GBR__max_depth': range(3,6), \n",
    "        'GBR__min_samples_split': range(2,10,2),\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBR\n",
      "{'GBR__n_estimators': range(300, 500, 30), 'GBR__subsample': [1], 'GBR__loss': ['absolute_error'], 'GBR__random_state': [1]}\n",
      "Best settings on step 0: {'GBR__loss': ['absolute_error'], 'GBR__n_estimators': [450], 'GBR__random_state': [1], 'GBR__subsample': [1]}\n",
      "Mae: -15879.68682350766\n",
      "{'GBR__loss': ['absolute_error'], 'GBR__n_estimators': [450], 'GBR__random_state': [1], 'GBR__subsample': [1], 'GBR__max_depth': range(3, 6), 'GBR__min_samples_split': range(2, 10, 2)}\n",
      "Best settings on step 1: {'GBR__loss': ['absolute_error'], 'GBR__n_estimators': [450], 'GBR__random_state': [1], 'GBR__subsample': [1], 'GBR__max_depth': [5], 'GBR__min_samples_split': [2]}\n",
      "Mae: -15675.482949972007\n"
     ]
    }
   ],
   "source": [
    "for pipe_name in consecutive_param_grids:\n",
    "    print(pipe_name)\n",
    "    pipe = pipelines[pipe_name]\n",
    "    n_steps = len(consecutive_param_grids[pipe_name])\n",
    "    best_params = {}\n",
    "    for step in range(n_steps):\n",
    "        # coarse step\n",
    "        sum_of_settings = {**best_params, **consecutive_param_grids[pipe_name][step]}\n",
    "        print(sum_of_settings)\n",
    "        coarse_search = GridSearchCV(pipe, param_grid=sum_of_settings, scoring='neg_mean_absolute_error')\n",
    "        coarse_search.fit(train_data, y)\n",
    "        for param_name in coarse_search.best_params_:\n",
    "            best_params[param_name] = [coarse_search.best_params_[param_name]]\n",
    "        print(f'Best settings on step {step}: {best_params}')\n",
    "        print(f'Mae: {coarse_search.best_score_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GBR__loss': 'absolute_error', 'GBR__n_estimators': 450, 'GBR__random_state': 1, 'GBR__subsample': 1, 'GBR__max_depth': 5, 'GBR__min_samples_split': 2}\n",
      "14566.346332718385\n"
     ]
    }
   ],
   "source": [
    "final_best_params = {}\n",
    "\n",
    "final_best_params['GBR'] = {\n",
    "    'GBR__loss': 'absolute_error', \n",
    "    'GBR__n_estimators': 450, \n",
    "    'GBR__random_state': 1, \n",
    "    'GBR__subsample': 1, \n",
    "    'GBR__max_depth': 5, \n",
    "    'GBR__min_samples_split': 2\n",
    "}\n",
    "\n",
    "for pipe_name in final_best_params:\n",
    "    pipe = pipelines[pipe_name]\n",
    "    pipe = pipe.set_params(**final_best_params[pipe_name])\n",
    "    pipe.fit(train_data, y)\n",
    "    final_predictions = pipe.predict(test_data)\n",
    "\n",
    "    output = pd.DataFrame({'Id': test_data.Id,\n",
    "                        'SalePrice': final_predictions})\n",
    "    output.to_csv(f'code/house-prices-competition/submission_{pipe_name}_pipeline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18f423941bc9d114fae7f04d212a56739c178c1e988574dac5cc94b8a9db688c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
