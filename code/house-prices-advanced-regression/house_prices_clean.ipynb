{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition where metric is rmse of log of prices\n",
    "[Competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data?select=train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up code checking\n",
    "# Set up filepaths\n",
    "import os\n",
    "os.chdir(os.path.join(os.path.expanduser('~'), 'kaggle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join('data', 'house-prices-advanced-regression', 'train.csv')\n",
    "train_data = pd.read_csv(train_path)\n",
    "\n",
    "test_path = os.path.join('data', 'house-prices-advanced-regression', 'test.csv')\n",
    "test_data_original = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select numerical and categorical variables, and separate target from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 78) (1459, 78)\n"
     ]
    }
   ],
   "source": [
    "y = np.log(train_data['SalePrice'])\n",
    "train_data.drop(columns=['SalePrice', 'Id'], inplace=True)\n",
    "test_data = test_data_original.drop(columns=['Id'])\n",
    "\n",
    "# remove stupid columns\n",
    "train_data.drop(columns=['LowQualFinSF'], inplace=True)\n",
    "test_data.drop(columns=['LowQualFinSF'], inplace=True)\n",
    "\n",
    "numerical_columns = train_data.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "numerical_columns.remove('MSSubClass')\n",
    "\n",
    "categorical_columns = [c for c in train_data.columns if c not in numerical_columns]\n",
    "\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some EDA and feature removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = train_data.hist(column=numerical_columns, figsize=(20, 20), rwidth=0.9)\n",
    "\n",
    "gaussable_columns = [\n",
    "    'LotFrontage',\n",
    "    'OverallQual',\n",
    "    'OverallCond',\n",
    "    'TotalBsmtSF',\n",
    "    '1stFlrSF', \n",
    "    '2ndFlrSF', \n",
    "    'GrLivArea',\n",
    "    'BedroomAbvGr',\n",
    "    'TotRmsAbvGrd',\n",
    "    'GarageArea'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add missing data col for every column with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from category_encoders import TargetEncoder \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# adds additional column which indicates whether data was missing for given feature\n",
    "class AddMissingIndicator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.num_cols_with_na = [] \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        df = pd.DataFrame(X)\n",
    "        self.num_cols_with_na = [c for c in df.columns if df[c].isna().any()]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        df_to_transform = pd.DataFrame(X)\n",
    "        for c in self.num_cols_with_na:\n",
    "            missing_col_name = f\"{c}_is_missing\"\n",
    "            df_to_transform[missing_col_name] = np.where(X[c].isna(), 1, 0) \n",
    "        \n",
    "        return df_to_transform \n",
    "\n",
    "\n",
    "target_cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')), \n",
    "    ('target_encoding', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# for cat boost\n",
    "ohe_cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')), \n",
    "    ('OHE', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# small cardinality columns get OHE, large get target encoding\n",
    "small_cardinality_cat_columns = [c for c in categorical_columns if len(set(train_data[c])) < 5]\n",
    "\n",
    "large_cardinality_cat_columns = [c for c in categorical_columns if c not in small_cardinality_cat_columns]\n",
    "\n",
    "normal_numeric_columns = [c for c in numerical_columns if c not in gaussable_columns]\n",
    "\n",
    "standard_num_pipeline = Pipeline(steps=[\n",
    "    ('missing_indicator', AddMissingIndicator()),\n",
    "    ('imputer', SimpleImputer(strategy='median')), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "gauss_num_pipeline = Pipeline(steps=[\n",
    "    ('missing_indicator', AddMissingIndicator()),\n",
    "    ('imputer', SimpleImputer(strategy='median')), \n",
    "    ('power_scaler', PowerTransformer())\n",
    "])\n",
    "\n",
    "preprocessing = ColumnTransformer(transformers=[\n",
    "    ('normal_numerical', standard_num_pipeline, normal_numeric_columns),\n",
    "    ('gauss_numerical', gauss_num_pipeline, gaussable_columns),\n",
    "    ('target_categorical', target_cat_pipeline, large_cardinality_cat_columns), \n",
    "    ('OHE_categorical', ohe_cat_pipeline, small_cardinality_cat_columns), \n",
    "])\n",
    "\n",
    "# preprocessing_no_ohe = ColumnTransformer(transformers=[\n",
    "#     ('normal_numerical', standard_num_pipeline, normal_numeric_columns),\n",
    "#     ('gauss_numerical', gauss_num_pipeline, gaussable_columns),\n",
    "#     ('categorical', cat_pipeline_no_ohe, categorical_columns)\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ridge\n",
      "RMSE: 0.1328379901771169\n",
      "\n",
      "Model: lasso\n",
      "RMSE: 0.3992282792085989\n",
      "\n",
      "Model: xgboost\n",
      "RMSE: 0.1394369142648041\n",
      "\n",
      "Model: lgbm\n",
      "RMSE: 0.13256667689915594\n",
      "\n",
      "Model: cat_boost\n",
      "RMSE: 0.11898162164812096\n",
      "\n",
      "Model: b_ridge\n",
      "RMSE: 0.12952290582498044\n",
      "\n",
      "Model: hist_gbr\n",
      "RMSE: 0.1319250093686956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge, BayesianRidge\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "models = {\n",
    "    'ridge': Ridge(),\n",
    "    'lasso': Lasso(random_state=1),\n",
    "    'xgboost': XGBRegressor(random_state=1, eval_metric='rmse'),\n",
    "    # 'GBR': GradientBoostingRegressor(loss='squared_error', random_state=1),\n",
    "    'lgbm': LGBMRegressor(), \n",
    "    'cat_boost': CatBoostRegressor(silent=True), \n",
    "    'b_ridge': BayesianRidge(), \n",
    "    'hist_gbr': HistGradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "pipelines = {key: Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    (f'{key}', models[key])\n",
    "]) for key in models}\n",
    "\n",
    "# models['cat_boost'] = CatBoostRegressor(silent=True)\n",
    "\n",
    "# pipelines['cat_boost'] = Pipeline(steps=[\n",
    "#     ('preprocessing_no_ohe', preprocessing_no_ohe), \n",
    "#     ('cat_boost', models['cat_boost'])\n",
    "# ])\n",
    "\n",
    "def get_rmse(predictor, X, y):\n",
    "    return -cross_val_score(predictor, X, y, scoring='neg_root_mean_squared_error', cv=5).mean()\n",
    "\n",
    "\n",
    "for p in pipelines:\n",
    "    predictor = pipelines[p]\n",
    "    mae = get_rmse(predictor, train_data, y)\n",
    "    print(f\"Model: {p}\")\n",
    "    print(f'RMSE: {mae}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV parameters for models with small number of params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from copy import deepcopy\n",
    "\n",
    "optuna.logging.set_verbosity(0)\n",
    "\n",
    "def ridge_cost(trial):\n",
    "    alpha = trial.suggest_loguniform('ridge__alpha', 10, 100)\n",
    "    tol = trial.suggest_loguniform('ridge__tol', 1e-7, 1e-4)\n",
    "    pipe = deepcopy(pipelines['ridge'])\n",
    "    pipe.set_params(**{\n",
    "        'ridge__alpha': alpha, \n",
    "        'ridge__tol': tol\n",
    "    })\n",
    "    return get_rmse(pipe, train_data, y)\n",
    "\n",
    "def lasso_cost(trial):\n",
    "    alpha = trial.suggest_loguniform('lasso__alpha', 1e-4, 1)\n",
    "    tol = trial.suggest_loguniform('lasso__tol', 1e-5, 1e-3)\n",
    "    pipe = deepcopy(pipelines['lasso'])\n",
    "    pipe.set_params(**{\n",
    "        'lasso__alpha': alpha, \n",
    "        # 'lasso__tol': tol\n",
    "    })\n",
    "    return get_rmse(pipe, train_data, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most important features for other models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "def xgboost_cost(trial: optuna.trial.Trial):\n",
    "    pipe = deepcopy(pipelines['xgboost'])\n",
    "\n",
    "    pipe.set_params(**{\n",
    "        'xgboost__max_depth': trial.suggest_int('max_depth', 1, 4), \n",
    "        'xgboost__min_child_weight': trial.suggest_int('min_cw', 0, 10), \n",
    "        'xgboost__colsample_bytree': trial.suggest_float('cs_bt', 0, 1),\n",
    "        'xgboost__n_estimators': trial.suggest_int('n_est', 50, 800), \n",
    "        'xgboost__reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
    "        'xgboost__subsample': trial.suggest_float('subsample', 0, 1), \n",
    "        'xgboost__reg_alpha': trial.suggest_float('alpha', 0, 1),\n",
    "        'xgboost__eval_metric': 'rmse'\n",
    "    })\n",
    "\n",
    "    return get_rmse(pipe, train_data, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lgbm_cost(trial: optuna.trial.Trial):\n",
    "#     pipe = deepcopy(pipelines['lgbm'])\n",
    "#     pipe.set_params(**{\n",
    "#         # \"lgbm__num_iterations\": trial.suggest_int('num_iterations', 50, 300),\n",
    "#         \"lgbm__learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "#         \"lgbm__num_leaves\": trial.suggest_int(\"num_leaves\", 20, 300),\n",
    "#         # \"lgbm__max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "#         \"lgbm__min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 10),\n",
    "#         # \"lgbm__max_bin\": trial.suggest_int(\"max_bin\", 200, 300),\n",
    "#         # \"lgbm__lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n",
    "#         # \"lgbm__lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n",
    "#         \"lgbm__min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "#         # \"lgbm__bagging_fraction\": trial.suggest_float(\n",
    "#         #     \"bagging_fraction\", 0.2, 0.95\n",
    "#         # ),\n",
    "#         # \"lgbm__bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "#         # \"lgbm__feature_fraction\": trial.suggest_float(\n",
    "#         #     \"feature_fraction\", 0.5, 0.95\n",
    "#         # ),\n",
    "#         'lgbm__early_stopping_rounds': 100,\n",
    "#         # 'lgbm__callbacks': [LightGBMPruningCallback(trial, 'rmse')]\n",
    "#     })\n",
    "\n",
    "#     return get_rmse(pipe, train_data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_cost(trial: optuna.trial.Trial):\n",
    "    pipe = deepcopy(pipelines['cat_boost'])\n",
    "    pipe.set_params(**{\n",
    "        \"cat_boost__colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1, log=True),\n",
    "        \"cat_boost__depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"cat_boost__boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "        \"cat_boost__bootstrap_type\": trial.suggest_categorical(\n",
    "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "        ),\n",
    "        \"cat_boost__eval_metric\": \"RMSE\",\n",
    "    })\n",
    "\n",
    "    return get_rmse(pipe, train_data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_ridge_cost(trial: optuna.trial.Trial):\n",
    "    pipe = deepcopy(pipelines['b_ridge'])\n",
    "    pipe.set_params(**{\n",
    "        'b_ridge__n_iter': trial.suggest_int('n_iter', 100, 1000, log=True), \n",
    "        'b_ridge__alpha_1': trial.suggest_float('alpha1', 1e-9, 1e-3, log=True),\n",
    "        'b_ridge__alpha_2': trial.suggest_float('alpha2', 1e-9, 1e-3, log=True),\n",
    "    })\n",
    "\n",
    "    return get_rmse(pipe, train_data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_gbr_cost(trial: optuna.trial.Trial):\n",
    "    pipe = deepcopy(pipelines['hist_gbr'])\n",
    "    pipe.set_params(**{\n",
    "        'hist_gbr__max_iter': trial.suggest_int('max_iter', 50, 500),\n",
    "        'hist_gbr__max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 5, 50), \n",
    "    })\n",
    "    return get_rmse(pipe, train_data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasil/earth-analytics-parent/earth-analytics/conda-env/lib/python3.9/site-packages/optuna/progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
      "  self._init_valid()\n",
      "100%|██████████| 20/20 [16:53<00:00, 50.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hist_gbr\n",
      "Best params: {'max_iter': 306, 'max_leaf_nodes': 5}\n",
      "Best score: 0.12736518003286176\n",
      "OrderedDict([('max_leaf_nodes', 0.9464325642567691), ('max_iter', 0.05356743574323084)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "functions = {\n",
    "    # 'ridge': ridge_cost, \n",
    "    # 'lasso': lasso_cost,\n",
    "    # 'xgboost': xgboost_cost, \n",
    "    # 'lgbm': lgbm_cost,\n",
    "    # 'cat_boost': catboost_cost,\n",
    "    # 'b_ridge': b_ridge_cost,\n",
    "    'hist_gbr': hist_gbr_cost,\n",
    "}\n",
    "\n",
    "for name in functions:\n",
    "    study = optuna.create_study()\n",
    "    study.optimize(functions[name], n_trials=20, show_progress_bar=True)\n",
    "    print(f'Model {name}')\n",
    "    print(f'Best params: {study.best_params}')\n",
    "    print(f'Best score: {study.best_value}')\n",
    "    print(optuna.importance.get_param_importances(study))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model xgboost: rmse 0.12474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasil/earth-analytics-parent/earth-analytics/conda-env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.867e-03, tolerance: 3.356e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/vasil/earth-analytics-parent/earth-analytics/conda-env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.043e-03, tolerance: 3.426e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/vasil/earth-analytics-parent/earth-analytics/conda-env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.157e-02, tolerance: 3.544e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model lasso: rmse 0.12614\n",
      "Model ridge: rmse 0.12931\n",
      "Model lgbm: rmse 0.13257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model cat_boost: rmse 0.11898\n",
      "Model b_ridge: rmse 0.12952\n",
      "Model hist_gbr: rmse 0.12737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasil/earth-analytics-parent/earth-analytics/conda-env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.867e-03, tolerance: 3.356e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/vasil/earth-analytics-parent/earth-analytics/conda-env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.043e-03, tolerance: 3.426e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/vasil/earth-analytics-parent/earth-analytics/conda-env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.157e-02, tolerance: 3.544e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble model rmse: 0.11902\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "final_best_params = {}\n",
    "\n",
    "final_best_params['xgboost'] = {\n",
    "    'xgboost__eval_metric': 'rmse', \n",
    "    # 'xgboost__max_depth': 2, \n",
    "    # 'xgboost__n_estimators': 418,\n",
    "    # 'xgboost__min_child_weight': 1, \n",
    "    # 'xgboost__gamma': 0.0, \n",
    "    # 'xgboost__colsample_bytree': 0.16, \n",
    "    # 'xgboost__subsample': 0.78, \n",
    "    # 'xgboost__reg_alpha': 0.37,\n",
    "    # 'xgboost__reg_lambda': 0.25\n",
    "\n",
    "    'xgboost__max_depth': 2, \n",
    "    'xgboost__min_child_weight': 3, \n",
    "    'xgboost__colsample_bytree': 0.2049030222473224, \n",
    "    'xgboost__n_estimators': 479, \n",
    "    'xgboost__reg_lambda': 0.2824862837800328, \n",
    "    'xgboost__subsample': 0.9561978086541947, \n",
    "    'xgboost__alpha': 0.8384411117820882\n",
    "}\n",
    "\n",
    "# final_best_params['lasso'] = {\n",
    "#     'lasso__alpha': 0.00048686724495962007, \n",
    "#     # 'lasso__tol': 1.8644218591611686e-05\n",
    "# }\n",
    "\n",
    "# final_best_params['GBR'] = {\n",
    "#     'GBR__loss': 'squared_error', \n",
    "#     'GBR__n_estimators': 380, \n",
    "#     'GBR__random_state': 1, \n",
    "#     'GBR__subsample': 1, \n",
    "#     'GBR__max_depth': 4, \n",
    "#     'GBR__min_samples_split': 8 \n",
    "# }\n",
    "\n",
    "final_best_params['ridge'] = {\n",
    "    'ridge__alpha': 10.013030924397276, \n",
    "    'ridge__tol': 5.943295651368705e-06\n",
    "}\n",
    "\n",
    "final_best_params['lgbm'] = {}\n",
    "\n",
    "final_best_params['cat_boost'] = {} # no improvement over default params\n",
    "\n",
    "final_best_params['b_ridge'] = {}\n",
    "\n",
    "final_best_params['hist_gbr'] = {\n",
    "    'hist_gbr__max_leaf_nodes': 5, \n",
    "    'hist_gbr__max_iter': 306\n",
    "}\n",
    "\n",
    "list_of_models = []\n",
    "scores = []\n",
    "for pipe_name in final_best_params:\n",
    "    # set the corresponding model with the optimal parameters\n",
    "    pipelines[pipe_name] = pipelines[pipe_name].set_params(**final_best_params[pipe_name])\n",
    "    pipe = pipelines[pipe_name]\n",
    "    list_of_models.append((pipe_name, pipe))\n",
    "    rmse = get_rmse(pipe, train_data, y)\n",
    "    scores.append(-rmse)\n",
    "    print(f'Model {pipe_name}: rmse {rmse:.5f}')\n",
    "\n",
    "weights = [min(scores)/score for score in scores]\n",
    "\n",
    "final_estimator = VotingRegressor(list_of_models, weights=weights)\n",
    "print(f'Ensemble model rmse: {get_rmse(final_estimator, train_data, y):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# final_best_params['lasso'] = {\n",
    "#     'lasso__alpha': 0.01, \n",
    "#     'lasso__tol': 0.0001\n",
    "# }\n",
    "\n",
    "print([i for i in final_best_params])\n",
    "all_estimators = [(name, pipelines[name]) for name in final_best_params]\n",
    "stacked_model = StackingRegressor(estimators=all_estimators)\n",
    "\n",
    "print('Stacked model')\n",
    "print(f'RMSE: {get_rmse(stacked_model, train_data, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate final submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459,) (1459,)\n"
     ]
    }
   ],
   "source": [
    "final_predictions = stacked_model.fit(train_data, y).predict(test_data)\n",
    "print(np.exp(final_predictions).shape, test_data_original['Id'].shape)\n",
    "output = pd.DataFrame({'Id': test_data_original.Id,\n",
    "                    'SalePrice': np.exp(final_predictions)})\n",
    "output.to_csv(f'code/house-prices-advanced-regression/big_ensemble_model_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18f423941bc9d114fae7f04d212a56739c178c1e988574dac5cc94b8a9db688c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
