{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition where metric is rmse of log of prices\n",
    "[Competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data?select=train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up code checking\n",
    "# Set up filepaths\n",
    "import os\n",
    "os.chdir(os.path.join(os.path.expanduser('~'), 'kaggle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join('data', 'house-prices-advanced-regression', 'train.csv')\n",
    "train_data = pd.read_csv(train_path)\n",
    "\n",
    "test_path = os.path.join('data', 'house-prices-advanced-regression', 'test.csv')\n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select numerical and categorical variables, and separate target from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n",
      "['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n"
     ]
    }
   ],
   "source": [
    "y = np.log(train_data['SalePrice'])\n",
    "train_data.drop(columns=['SalePrice', 'Id'], inplace=True)\n",
    "test_data.drop(columns=['Id'], inplace=True)\n",
    "\n",
    "numerical_columns = train_data.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "numerical_columns.remove('MSSubClass')\n",
    "\n",
    "categorical_columns = [c for c in train_data.columns if c not in numerical_columns]\n",
    "\n",
    "print(numerical_columns)\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some EDA and feature removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1460.000000\n",
      "mean        5.844521\n",
      "std        48.623081\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max       572.000000\n",
      "Name: LowQualFinSF, dtype: float64\n",
      "26\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = train_data.hist(column=numerical_columns, figsize=(20, 20), rwidth=0.9)\n",
    "\n",
    "# remove stupid columns\n",
    "print(train_data['LowQualFinSF'].describe())\n",
    "print(train_data[train_data['LowQualFinSF'] > 0]['LowQualFinSF'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add missing data col for every column with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# adds additional column which indicates whether data was missing for given feature\n",
    "class AddMissingIndicator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numeric_columns):\n",
    "        self.numeric_columns = numeric_columns\n",
    "        self.num_cols_with_na = [] \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.num_cols_with_na = [c for c in self.numeric_columns if X[c].isna().any()]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        for c in self.num_cols_with_na:\n",
    "            missing_col_name = f\"{c}_is_missing\"\n",
    "            X[missing_col_name] = np.where(X[c].isna(), 1, 0)\n",
    "        \n",
    "        return X \n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')), \n",
    "    ('OHE', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('missing_indicator', AddMissingIndicator(numerical_columns)),\n",
    "    ('imputer', SimpleImputer(strategy='mean')), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer(transformers=[\n",
    "    ('numerical', num_pipeline, numerical_columns),\n",
    "    ('categorical', cat_pipeline, categorical_columns)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ridge\n",
      "RMSE: 0.18582366294558147\n",
      "\n",
      "Model: lasso\n",
      "RMSE: 0.396519083740399\n",
      "\n",
      "Model: xgboost\n",
      "RMSE: 0.12339455502154234\n",
      "\n",
      "Model: GBR\n",
      "RMSE: 0.11893758248241178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "models = {\n",
    "    'ridge': Ridge(),\n",
    "    'lasso': Lasso(random_state=1),\n",
    "    'xgboost': XGBRegressor(random_state=1, eval_metric='rmse'),\n",
    "    'GBR': GradientBoostingRegressor(loss='squared_error', random_state=1),\n",
    "}\n",
    "\n",
    "pipelines = {key: Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    (f'{key}', models[key])\n",
    "]) for key in models}\n",
    "\n",
    "def get_rmse(predictor, X_train, y_train, X_test, y_test):\n",
    "    predictor.fit(X_train, y_train)\n",
    "    predictions = predictor.predict(X_test)\n",
    "    return mean_squared_error(predictions, y_test, squared=False) # to return rmse and not mse\n",
    "\n",
    "\n",
    "for p in pipelines:\n",
    "    predictor = pipelines[p]\n",
    "    mae = get_rmse(predictor, X_train, y_train, X_test, y_test)\n",
    "    print(f\"Model: {p}\")\n",
    "    print(f'RMSE: {mae}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV parameters for models with small number of params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_small_model = {}\n",
    "\n",
    "param_grid_small_model['ridge'] = {\n",
    "    'ridge__alpha': np.logspace(1.5, 1.8, num=11),\n",
    "    'ridge__tol': np.logspace(-6, -5, num=11)\n",
    "}\n",
    "\n",
    "param_grid_small_model['lasso'] = {\n",
    "    'lasso__alpha': np.logspace(-1, 1, num=11),\n",
    "    'lasso__max_iter': [1000],\n",
    "    'lasso__tol': np.logspace(-4, -3.5, num=2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: ridge\n",
      "Best values: {'ridge__alpha': 31.622776601683793, 'ridge__tol': 1e-06}\n",
      "Best score MAE: -0.13869308083619108\n",
      "\n",
      "Model name: lasso\n",
      "Best values: {'lasso__alpha': 0.1, 'lasso__max_iter': 1000, 'lasso__tol': 0.00031622776601683794}\n",
      "Best score MAE: -0.22100018969344037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "for model_name in param_grid_small_model:\n",
    "    print(f\"Model name: {model_name}\")\n",
    "    pipe = pipelines[model_name]\n",
    "    search = GridSearchCV(estimator=pipe, param_grid=param_grid_small_model[model_name], scoring='neg_root_mean_squared_error')\n",
    "    search.fit(train_data, y)\n",
    "    print(f'Best values: {search.best_params_}')\n",
    "    print(f'Best score MAE: {search.best_score_}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search for models with many params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_param_grids = {}\n",
    "consecutive_param_grids['xgboost'] = [ \n",
    "    {\n",
    "        'xgboost__max_depth':range(1,5),\n",
    "        'xgboost__min_child_weight':range(1,5,1), \n",
    "        'xgboost__eval_metric': ['rmse']\n",
    "    }, \n",
    "    {\n",
    "        'xgboost__gamma':[i/10.0 for i in range(0,2)]\n",
    "    },\n",
    "    {\n",
    "        'xgboost__subsample':[i/10.0 for i in range(6,11)],\n",
    "        'xgboost__colsample_bytree':[i/10.0 for i in range(2,9)]\n",
    "    }, \n",
    "    {\n",
    "        'xgboost__reg_alpha':[1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "consecutive_param_grids['GBR'] = [\n",
    "    {\n",
    "        'GBR__n_estimators': range(100, 300, 30), \n",
    "        'GBR__subsample': [1],\n",
    "        'GBR__loss': ['squared_error'],\n",
    "        'GBR__random_state': [1]\n",
    "    }, \n",
    "    {\n",
    "        'GBR__max_depth': range(1,4), \n",
    "        'GBR__min_samples_split': range(8,20,2),\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost\n",
      "{'xgboost__max_depth': range(1, 5), 'xgboost__min_child_weight': range(1, 5), 'xgboost__eval_metric': ['rmse']}\n",
      "Best settings on step 0: {'xgboost__eval_metric': ['rmse'], 'xgboost__max_depth': [2], 'xgboost__min_child_weight': [1]}\n",
      "RMSE: -0.1291500712230799\n",
      "{'xgboost__eval_metric': ['rmse'], 'xgboost__max_depth': [2], 'xgboost__min_child_weight': [1], 'xgboost__gamma': [0.0, 0.1]}\n",
      "Best settings on step 1: {'xgboost__eval_metric': ['rmse'], 'xgboost__max_depth': [2], 'xgboost__min_child_weight': [1], 'xgboost__gamma': [0.0]}\n",
      "RMSE: -0.1291500712230799\n",
      "{'xgboost__eval_metric': ['rmse'], 'xgboost__max_depth': [2], 'xgboost__min_child_weight': [1], 'xgboost__gamma': [0.0], 'xgboost__subsample': [0.6, 0.7, 0.8, 0.9, 1.0], 'xgboost__colsample_bytree': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]}\n",
      "Best settings on step 2: {'xgboost__eval_metric': ['rmse'], 'xgboost__max_depth': [2], 'xgboost__min_child_weight': [1], 'xgboost__gamma': [0.0], 'xgboost__colsample_bytree': [0.6], 'xgboost__subsample': [0.8]}\n",
      "RMSE: -0.1291500712230799\n",
      "{'xgboost__eval_metric': ['rmse'], 'xgboost__max_depth': [2], 'xgboost__min_child_weight': [1], 'xgboost__gamma': [0.0], 'xgboost__colsample_bytree': [0.6], 'xgboost__subsample': [0.8], 'xgboost__reg_alpha': [1e-07, 1e-06, 1e-05, 0.0001, 0.001]}\n",
      "Best settings on step 3: {'xgboost__eval_metric': ['rmse'], 'xgboost__max_depth': [2], 'xgboost__min_child_weight': [1], 'xgboost__gamma': [0.0], 'xgboost__colsample_bytree': [0.6], 'xgboost__subsample': [0.8], 'xgboost__reg_alpha': [1e-07]}\n",
      "RMSE: -0.1291498982246709\n",
      "GBR\n",
      "{'GBR__n_estimators': range(100, 300, 30), 'GBR__subsample': [1], 'GBR__loss': ['squared_error'], 'GBR__random_state': [1]}\n",
      "Best settings on step 0: {'GBR__loss': ['squared_error'], 'GBR__n_estimators': [190], 'GBR__random_state': [1], 'GBR__subsample': [1]}\n",
      "RMSE: -0.12442730984269043\n",
      "{'GBR__loss': ['squared_error'], 'GBR__n_estimators': [190], 'GBR__random_state': [1], 'GBR__subsample': [1], 'GBR__max_depth': range(1, 4), 'GBR__min_samples_split': range(8, 20, 2)}\n",
      "Best settings on step 1: {'GBR__loss': ['squared_error'], 'GBR__n_estimators': [190], 'GBR__random_state': [1], 'GBR__subsample': [1], 'GBR__max_depth': [3], 'GBR__min_samples_split': [14]}\n",
      "RMSE: -0.12437165590214172\n"
     ]
    }
   ],
   "source": [
    "for pipe_name in consecutive_param_grids:\n",
    "    print(pipe_name)\n",
    "    pipe = pipelines[pipe_name]\n",
    "    n_steps = len(consecutive_param_grids[pipe_name])\n",
    "    best_params = {}\n",
    "    for step in range(n_steps):\n",
    "        # coarse step\n",
    "        sum_of_settings = {**best_params, **consecutive_param_grids[pipe_name][step]}\n",
    "        print(sum_of_settings)\n",
    "        coarse_search = GridSearchCV(pipe, param_grid=sum_of_settings, scoring='neg_root_mean_squared_error')\n",
    "        coarse_search.fit(train_data, y)\n",
    "        for param_name in coarse_search.best_params_:\n",
    "            best_params[param_name] = [coarse_search.best_params_[param_name]]\n",
    "        print(f'Best settings on step {step}: {best_params}')\n",
    "        print(f'RMSE: {coarse_search.best_score_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model xgboost: RMSE 0.11546850816738524\n",
      "Model xgboost: MAE 0.08257986640152633\n",
      "Model GBR: RMSE 0.11654589767858496\n",
      "Model GBR: MAE 0.07806801643021383\n"
     ]
    }
   ],
   "source": [
    "final_best_params = {}\n",
    "\n",
    "final_best_params['xgboost'] = {\n",
    "    'xgboost__eval_metric': 'rmse', \n",
    "    'xgboost__max_depth': 2, \n",
    "    'xgboost__min_child_weight': 1, \n",
    "    'xgboost__gamma': 0.0, \n",
    "    'xgboost__colsample_bytree': 0.6, \n",
    "    'xgboost__subsample': 0.8, \n",
    "    'xgboost__reg_alpha': 0.0001\n",
    "}\n",
    "\n",
    "final_best_params['GBR'] = {\n",
    "    'GBR__loss': 'squared_error', \n",
    "    'GBR__n_estimators': 190, \n",
    "    'GBR__random_state': 1, \n",
    "    'GBR__subsample': 1, \n",
    "    'GBR__max_depth': 3, \n",
    "    'GBR__min_samples_split': 10\n",
    "\n",
    "}\n",
    "\n",
    "for pipe_name in final_best_params:\n",
    "    pipe = pipelines[pipe_name]\n",
    "    pipe = pipe.set_params(**final_best_params[pipe_name])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    final_predictions = pipe.predict(X_test)\n",
    "\n",
    "    print(f'Model {pipe_name}: RMSE {mean_squared_error(y_test, final_predictions, squared=False)}')\n",
    "    print(f'Model {pipe_name}: MAE {mean_absolute_error(y_test, final_predictions)}')\n",
    "# for pipe_name in final_best_params:\n",
    "#     pipe = pipelines[pipe_name]\n",
    "#     pipe = pipe.set_params(**final_best_params[pipe_name])\n",
    "#     pipe.fit(train_data, y)\n",
    "#     final_predictions = np.exp(pipe.predict(test_data))\n",
    "\n",
    "#     output = pd.DataFrame({'Id': test_data.Id,\n",
    "#                         'SalePrice': final_predictions})\n",
    "#     output.to_csv(f'code/house-prices-competition/{pipe_name}_pipeline_rmse.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find actual eval metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                    'SalePrice': [0 for i in range(len(test_data['Id']))]})\n",
    "output.to_csv(f'code/house-prices-competition/null.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rmse: 197583.57145209657\n",
      "MAE: 180921.19589041095\n"
     ]
    }
   ],
   "source": [
    "predicted_rmse = (np.sum(np.exp(y)**2)/len(y))**0.5\n",
    "print(f'Rmse: {predicted_rmse}')\n",
    "\n",
    "predicted_mae = np.sum(np.exp(y))/len(y)\n",
    "print(f'MAE: {predicted_mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18f423941bc9d114fae7f04d212a56739c178c1e988574dac5cc94b8a9db688c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
